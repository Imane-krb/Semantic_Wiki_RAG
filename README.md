# ğŸ” RAG System â€“ Digital Twin & Building Energy

A **Retrieval-Augmented Generation (RAG)** system that ingests structured knowledge from a MediaWiki ontology about digital twin technology and building energy consumption, retrieves relevant content per user query, generates grounded answers via an Ollama LLM, and logs full traceability for every interaction.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MediaWiki   â”‚â”€â”€â”€â”€â–¶â”‚  Ingestion    â”‚â”€â”€â”€â”€â–¶â”‚  Embedding   â”‚
â”‚  (Ontology)  â”‚     â”‚  (API Parse)  â”‚     â”‚  (MiniLM)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit   â”‚â—€â”€â”€â”€â”‚  Generation   â”‚â—€â”€â”€â”€â”‚  ChromaDB    â”‚
â”‚  (Web UI)    â”‚    â”‚  (Ollama LLM) â”‚    â”‚  (VectorDB)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Traceability Logger              â”‚
â”‚  (JSON traces: queryâ†’retrievalâ†’answer)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Ingest**: Fetch all pages from MediaWiki API â†’ Parse templates (Article, Author, Institution, Keyword) â†’ Build rich-text documents
2. **Embed**: Chunk documents (1000 chars, 200 overlap) â†’ Generate embeddings (all-MiniLM-L6-v2) â†’ Store in ChromaDB
3. **Query**: User asks question â†’ Embed query â†’ Semantic search in ChromaDB â†’ Retrieve top-k chunks
4. **Generate**: Build grounded prompt (system instruction + context + query) â†’ Send to Ollama â†’ Return answer
5. **Trace**: Log entire flow (query, retrieved docs, prompt, response, latency) to JSON file

---

## ğŸ“‚ Project Structure

```
Partie RAG/
â”œâ”€â”€ app.py                      # Streamlit web interface
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Configuration (wiki URL, Ollama URL)
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Centralized settings
â”‚   â”œâ”€â”€ ingestion.py            # MediaWiki data extraction
â”‚   â”œâ”€â”€ embeddings.py           # Text chunking + embedding
â”‚   â”œâ”€â”€ vector_store.py         # ChromaDB vector store
â”‚   â”œâ”€â”€ retrieval.py            # Semantic search
â”‚   â”œâ”€â”€ generation.py           # Ollama LLM generation
â”‚   â”œâ”€â”€ traceability.py         # JSON trace logging
â”‚   â””â”€â”€ rag_pipeline.py         # Pipeline orchestrator
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chroma_db/              # Persisted vector store
â””â”€â”€ logs/
    â””â”€â”€ traces/                 # JSON trace files (one per query)
```

---

## ğŸš€ Installation & Setup

### Prerequisites

- Python 3.9+
- Access to the MediaWiki instance
- Ollama running with a model (e.g., `mistral`)

### Install Dependencies

```bash
cd "Partie RAG"
pip install -r requirements.txt
```

### Configure

Edit `.env` if URLs differ from defaults:

```
MEDIAWIKI_URL=http://10.3.17.196:8080
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_MODEL=mistral
```

---

## ğŸ’» Usage

### Web Interface (Recommended)

```bash
streamlit run app.py
```

1. Click **"Ingest from MediaWiki"** in the sidebar to load data
2. Type a question in the input box
3. View the answer, retrieved sources, and full traceability

### Programmatic Usage

```python
from src.rag_pipeline import RAGPipeline

# Initialize
pipeline = RAGPipeline()

# Ingest data from MediaWiki
pipeline.ingest()

# Ask a question
result = pipeline.query("What is digital twin technology in building energy?")
print(result["answer"])
print(f"Sources: {len(result['sources'])}")
print(f"Trace ID: {result['trace_id']}")
```

---

## ğŸ”— Traceability

Every query generates a JSON trace file in `logs/traces/`, containing:

| Field                 | Description                   |
| --------------------- | ----------------------------- |
| `trace_id`            | Unique identifier (UUID)      |
| `timestamp`           | ISO 8601 timestamp            |
| `user_query`          | Original question             |
| `retrieved_documents` | Chunks retrieved with scores  |
| `constructed_prompt`  | Full prompt sent to LLM       |
| `llm_response`        | Generated answer              |
| `model_used`          | LLM model name                |
| `latency_ms`          | Retrieval + generation timing |

Example trace location: `logs/traces/trace_abc123.json`

---

## ğŸ§© Components

| Component        | Technology                     | Purpose                                               |
| ---------------- | ------------------------------ | ----------------------------------------------------- |
| Knowledge Source | MediaWiki API                  | Structured ontology (articles, authors, institutions) |
| Embeddings       | sentence-transformers (MiniLM) | Semantic vector representations                       |
| Vector Store     | ChromaDB                       | Persistent similarity search                          |
| LLM              | Ollama (Mistral)               | Grounded answer generation                            |
| Traceability     | JSON files                     | Full pipeline logging                                 |
| Interface        | Streamlit                      | Interactive web UI                                    |

---

## ğŸ“Š MediaWiki Ontology

The knowledge base contains ~400 pages with 4 entity types:

- **Articles**: Research papers about digital twins, building energy, HVAC optimization (fields: title, abstract, DOI, authors, keywords, citation metrics)
- **Authors**: Researchers with affiliations, h-index, publication counts
- **Institutions**: Universities and organizations with country and type
- **Keywords**: Research topics linking articles together

---

## ğŸ‘¥ Team

Developed as part of the Semantic Web + Advanced Databases project.
